      What is Devops ?
Ans : DevOps is  a combination of Development and Operations and is a set of practices, principles, and cultural philosophy.
      DevOps is a software development methodology that accelerates the delivery of higher-quality applications and services by combining and automating the 
      work of software development and IT operations teams. DevOps is a set of practices, tools, and a cultural philosophy that automate and integrate the 
      processes between software development and IT teams.

      What is Cycle of Devops ?
Ans : The DevOps cycle typically involves several key stages, each focused on different aspects of the software development and deployment process. Here's a 
      high-level overview:

      1. Plan: In this stage, teams define the objectives, requirements, and scope of the project. This involves gathering feedback from stakeholders, 
      prioritizing tasks, and creating a roadmap for development.

      2. Code: Developers write and review code to implement new features or fix bugs. Version control systems like Git are often used to manage changes, 
      and collaboration tools facilitate code reviews and discussions among team members.

      3. Build: The code is compiled, built, and packaged into deployable artifacts. Automation tools are frequently used to streamline this process and 
      ensure consistency across different environments.

      4. Test: Automated testing is performed to verify the functionality, performance, and security of the application. This includes unit tests, 
      integration tests, and end-to-end tests to catch bugs and regressions early in the development cycle.

      5. Deploy: The application is deployed to various environments, such as development, testing, staging, and production. Continuous integration and 
      continuous delivery (CI/CD) pipelines automate the deployment process, allowing for frequent and reliable releases.

      6. Operate: Once the application is live, operations teams monitor its performance, availability, and security. They may also handle user support, 
      troubleshooting, and incident response to ensure smooth operation and minimize downtime.

      7. Monitor: Monitoring tools collect and analyze data on various aspects of the application, including performance metrics, error logs, and user 
      behavior. This feedback loop provides insights into the application's health and helps identify areas for improvement.

      8. Feedback: Feedback from monitoring, user engagement, and stakeholder input is used to inform future iterations of the application. This may 
      involve refining features, optimizing performance, or addressing user concerns to continuously improve the product.

      What is monolithic architeture and microlithic architecture ?
Ans : In a monolithic architecture, an entire application is designed and deployed as a single unit. All components and functionalities of the application are 
      tightly coupled and interconnected within the same codebase. Typically, monolithic applications consist of three main components: the user interface 
      (frontend), business logic (backend), and data storage (database). These components are often developed using a single programming language and deployed 
      on a single server or server cluster.

      Microservices architecture, on the other hand, is an architectural style where an application is composed of small, independent services that 
      communicate with each other through well-defined APIs. Each service is responsible for a specific business capability and can be developed, deployed, 
      and scaled independently of other services. This approach enables teams to adopt different technologies, programming languages, and frameworks for each 
      service, allowing for greater flexibility and agility.

      What is git ? 
Ans : Git is the distributed version control system, that tracks the history of changes as people 
      and teams collaborate on projects. 
  
      (Commands of Git):

      git init
      ls -a
      git branch
      git add dev
      git status
      git commit -m "first commit" 
      git status
      git branch main
      git checkout dev 
      git checkout -b
      git branch 
      git push origin main
      git merge main.

      What is Pull Request ?
Ans : When working as a team on a project, it is important that everyone stays up to date.
      Any time you start working on a project, you should get the most recent changes to your local copy.
      With Git, you can do that with pull.
      pull is a combination of 2 different commands:
      > fetch : fetch gets all the change history of a tracked branch/repository.
      > merge : merge combines the current branch, with a specified branch.

      What is the difference between git pull and git fetch ?
Ans : Git Fetch is the command that tells your local git to retrieve the latest meta-data info from the original (yet doesn’t do any file transferring.
      It’s more like just checking to see if there are any changes available).
      You can use git fetch to know the changes done in the remote repo/branch since your last pull.

      Git Pull on the other hand does that and brings (copy) those changes from the remote repository.

      Difference between Merge and Rebase ? 
Ans : Merge:
      When you merge branches in Git, you're essentially combining the changes from one branch into another.
      This creates a new commit on the target branch, often with a merge commit that records the integration of changes.
      Merge preserves the commit history of both branches, showing the branch divergence and subsequent merging.
      It's suitable for preserving the context of how and when the changes were integrated.
      Merge commits can clutter the history, especially in projects with frequent merges.

      Rebase:
      Rebase, on the other hand, rewrites the commit history by moving the entire feature branch to the tip of the branch you're rebasing onto. 
      It essentially changes the base of the feature branch.
      It results in a linear history as if the work had been started from the current state of the target branch.
      Rebasing can make the commit history cleaner and easier to understand since it eliminates unnecessary merge commits.
      It's great for creating a cleaner history before pushing changes to a shared repository or before creating a pull request.

      What is Docker ?
Ans : Docker is a platform that enables developers to build, package, distribute, and run applications using containers.
      Containers allow developers to pack an application and its dependencies into a standardized unit, ensuring that it runs consistently across different environments.
      Docker provides tools and a runtime environment for managing these containers. Docker is a container runtime tool.

      docker client,  docker host ,   docker registrie 
       docker run                     image, nginx, tomcat
       docker build 
       docker pull


commands of docker 
 docker --help
docker run -it, (is used to login) -d (python)
sudo docker ps (to see running container)
sudo docker ps-a (show list of all containers)
sudo docker start (container ID)
docker run -d (to run in foreground)
sudo docker diff (see the chnges made in conatiner)
sudo docker commit (running image)
Vi dockerfile
FROM (ubuntu:latest)
ENV myname vedant 
WORKDIR /mnt 
COPY . . (it copies from local host machine to container) 
ADD 
RUN echo " hello world " > /mnt/filename
CMD 
sudo docker build -t imagename .
echo $ is a variable

      What is the use of entrypoint instruction ?
Ans :  Specify default executable )ENTRYPOINT lets you set the container’s primary purpose, like running a web server, database, or application. 
       It also allows you to pass arguments at runtime to customize the container’s behavior.


      What is kubernetes ?
Ans : It is used to manage container and deploy services. 
      It is a tool that helps us to run and manage applications in containers.
      k8s works on cluster architecture, master and worker nodes.


Architecture of Kubernete :

 Master plane 

 Api comminucates with master control plane and worker node.

 Scheduler worker node me pods schedule krta hai.

 Controller manager is use to manage pods according to resources requirement. desired state should match active state

 Etcd is database cluster metadata jo master node ke bahar hai isme pods ki info component ki info rehati hai.


 worker node 
 There are multiple pods in worker node, pods is a container manager tool.
 There are containers in a pod.
 Docker is a container runtime.
 The Kubelet works closely with the master control plane to start, stop, 
 and manage containers based on Pod specifications.
 Kube proxy provides the routing traffic and load balancers.

 This is a question asked in interview.
 {if my pod is crashed what will be the next step in kubernetes architecture.}

What is Kubernetes?
==> 
Kubernetes is an open-source Container Management tool that automates container deployment, container scaling, descaling, and container load balancing (also called a container orchestration tool).
Kubernetes is an open-source platform that manages Docker containers in the form of a cluster. Along with the automated deployment and scaling of containers, it provides healing by automatically 
restarting failed containers and rescheduling them when their hosts die. This capability improves the application’s availability.

Features of Kubernetes:
1.Automated Scheduling– Kubernetes provides an advanced scheduler to launch containers on cluster nodes. It performs resource optimization.
2.Self-Healing Capabilities– It provides rescheduling, replacing, and restarting the containers that are dead.
3.Automated Rollouts and Rollbacks– It supports rollouts and rollbacks for the desired state of the containerized application.
4.Horizontal Scaling and Load Balancing– Kubernetes can scale up and scale down the application as per the requirements.
5.Resource Utilization– Kubernetes provides resource utilization monitoring and optimization, ensuring containers are using their resources efficiently.
6.Support for multiple clouds and hybrid clouds– Kubernetes can be deployed on different cloud platforms and run containerized applications across multiple clouds.

/use/local/bin => contains all the binary and executable files.

kubernetes Architecture:
In kubernetes architecture there is cluster. In cluster there are two planes as follows: 

{1} Master Control Plane     
In master plane there are 4 components:
1. API-Server : It validates the request and authenticates the user.  It exposes the Kubernetes API, which allows users, administrators, and other components to communicate with the cluster.
2. Controller Manager : is used to monitor and make sure that every thing is working as we have define and will compare it with desire state
3. Scheduler :The Scheduler is responsible for placing Pods onto suitable worker nodes. 
4. etcd : it stores the metadata of all components.It holds the configuration data and the state of the entire cluster.

{2} Worker Node Plane
1. Kubelet :  The Kubelet is an agent that runs on each worker node and communicates with the master control plane.The Kubelet works closely with the master control
   plane to start, stop, and manage containers based on Pod specifications.
2. kube-Proxy :  Kube Proxy sets up routing and load balancing so that applications can seamlessly communicate with each other and external resources.
3. Container runtime : A container runtime is the software responsible for running containers on the worker nodes.

Difference between pod,container and deployment.
Difference between deployment and replicaset.
Difference between imperative and declarative approach .
what is yaml? => yet another markup language

In terms of kubernetes pod is a yamal manifest

commands :-
-kubectl get all    (shows all services) 
-kubectl get namespaces  
   There are default 4 namespaces:
    default
    kube-node-lease
    kube-public
    kube-system
-
-kubectl get namespaces
-kubectl get pods -n <namespaces>
To create namespaces:
kubectl create namespace -n CDECB
vim mynamespace.yml

kubectl create -f mynamespace.yml

vim pod.yml
kubectl create -f pod.yml

kubectl get po  
kubectl describe po nginx


Q Create a pod in a namespace
=> kubectl run nginx --image=nginx -n default --restart=Never

Q Create a pod and expose it
=> kubectl run nginx --image=nginx --restart=Never --port=80

Q Upgrade a deployment by using rolling update
=> First create a deployment yaml file
=> kubectl apply -f deployment.yaml
=> Make change in the version of the image in the yaml file.
=> kubectl apply -f deployment.yaml

# check the status of rollout.
=> kubectl rollout statuts deployment <deployment name>
#check the revision of deployment.
=> kubectl rollout history deployment <deployment name>

Q Create a pod with a command
=> kubectl run nginx --image=nginx --restart=Never

Q Taint a node to be unschedulable
=> kubectl taint nodes node01 key1=value:NoSchedule


Q List all the internal IP's of all the nodes in the cluster
=> kubectl get nodes -o wide

Q Create a secret and mount it to the pod
=>

Q Show the logs from the container
=> kubectl logs <pod name>

Q Overwrite the label of the pod with a value
=> kubectl label pod <pod name> <label key=value> --overwrite 
=> kubectl get pods --show-labels

Q Find out which pods are available with the label in the cluster
=> kubectl get pods --show-labels

Q Create a pod that will only be scheduled on a node with a specific label


Q Remove the taint added to the node
=> kubectl taint nodes node01 key1=value:taint_Effect-

Q Schedule a pod on the node by using tolerations
=> Create a yaml file for pod toleration
kind: Pod
apiVersion: v1
metadata:
  name: mypod
spec:
  containers:
  - name: nginx
    image: nginx
  tolerations:
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoSchedule"

=> kubectl apply -f yamlfile

# Add a taint to the node
=> Kubectl taint nodes node01 key1=value:NoSchedule

Q Create a replicaset which has 3 replicas
# create a replicaset yaml file
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
  labels:
    app: myapp
    type: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
      type: frontend
  template:
    metadata:
      name: nginx
      labels:
        app: nginx
        type: frontend
    spec:
      containers:
      - name: nginx
        image: nginx:latest

=> kubectl apply -f replicaset.yaml
# we can also scale up and down replicas using command
=> kubectl scale replicaset <replica name> --replicas=<replica number>


Q Create a replicaset which has 3 replicas
=> create a replica.yml file.
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest

=> Now apply it 
$ kubectl apply -f replicaset.yaml


Q Edit an existing pod and add a command
=> Fisrt we have to delete pod then make changes in the yaml file. and re apply it.

Q Delete all the pods with a specific label
=> kubectl delete pod -l <label key=value>

Q Create a pod and set "NET_ADMIN"
=> First create a pod.yml file
apiVersion: v1
kind: Pod
metadata:
  name: netadmin-pod
spec:
  containers:
  - name: netadmin-container
    image: nginx
    securityContext:
      capabilities:
        add:
        - NET_ADMIN
 
 $ k apply -f pod.yml
 => K describe pod <podname>

 Q Get a list of all the pods which were recently deleted
=> Kubeclt get events 

Q Delete a pod without any delay
=> kubectl delete pod <podname> --grace-period=0

Q Find the static pod path
=> cd /etc/kubernetes/manifests

Q Create a clusterrole, service account and rolebinding
=> Create a file of cluster role.yml file.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: secret-reader
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]

=> Now apply kubectl apply -f clusterrole.yml
kubectl get clusterrole secret-reader

=>  Create service account yaml file service.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: example-serviceaccount

=> create a rolebinding.yaml file
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: read-secrets-global
subjects:
- kind: User
  name: riya
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io

                                              
                                             ||   TERRAFORM   ||

vs code me folder banana terraform naam ka 
main.tf k naam se repo banana hai 

terraform init (initialize krenga) 
terraform plan 
terraform destroy (sab resource delete kr denga .tf files me k) 

multiple providers aur multiple regions
 
variables in terraform (A string where multiple values are added is called Variable)

to set value we create a file tf.var

Input variable (jo bhi value pass karna honga to vo isme karna padenga)

Output variable (resource ki value print karta hai)

provider "aws" {
region = "us-east"
}
resource "aws_instance" "my instance name" {
ami = .....
instance_type = "t2 micro"
}

resource "S3" "my bucket" {
ami = .....
instance type = ....
}

resource ...
ami
instance
logs = var.logs



  whenever the infrastructure is modified terraform uses state file to know the user what is done in that because terraform does not
  have direct access, all the sensitive info is stored which are very imp as per the company or org.


Q. How do you manage satefile ?
  Ans :  We save in remote aws backend because humne versoning enable kr k rakhi hai
         and i may use as a backup when my infrastructure is destroyed.




                                         || JENKINS ||



Ansible  
app level pe use hota h , instance login services install krne k liye use hota hai.

provisioners 
file 
local ecec
remote exec
 

      Why do u need a branching strategy in devops ?

Ans : A branching strategy is something a software development team uses when interacting with a version control system for writing and managing code.
      as the name suggests, the branching strategy focuses on how branches are used in the development process.
      A branching strategy helps define how the delivery team functions and how each feature, improvement, or bug fix is handled.
      It also reduces the complexity of the delivery pipeline by allowing developers to focus on developments and deployments only on the relevant branches—without affecting the entire product.

      What are the different strategy in devops ?

 Ans : Gitflow, Githubflow, Trunk based development, Gitlab flow.

      plan code build comes in continuous integration
      test release comes under continuous development 
      deploy comes under contionious delivery 
      jenkins port number 8080


Continous Delivery : 
 The primary goal of Continuous Delivery is to enable software teams to release new features, 
 bug fixes, and improvements in a rapid and consistent manner.
 In this the version control system like Git is used to manage the change in the codebase and used 
 to rollback if necessary.
 Continuous improvement is used.


Continuous Deployment (CD) :
 Continuous Deployment is a software development and DevOps practice where every code change that passes automated tests
 and quality checks is automatically deployed to production without any manual intervention. 
 Continous monitoring is used to detect issues or anomalies as soon as they occur.
 Automation and continuous integration is used.


what is jar file in java and var file 

Tomcat is a application server based tool 
It is java based tool. 

SDLC (software dev life cycle) pull, build, test, deploy.
sabse pahele project create honga fr build krne k baad job ban jayenga.
controller is master jenkins
DSL is domain specific language


What plugins are used in jenkins 

Lambda test automation is a plugin for seleium testing.






